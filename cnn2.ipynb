{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import librosa as lb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_33456\\3390396274.py:43: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  soundArr, sample_rate = lb.load(path)\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/rachelwang/Downloads/bids_with_sensitive_recordings/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab/ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8/audio/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab_ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8_Diadochokinesis_rec-Diadochokinesis-buttercup.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening '/Users/rachelwang/Downloads/bids_with_sensitive_recordings/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab/ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8/audio/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab_ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8_Diadochokinesis_rec-Diadochokinesis-buttercup.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 85\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m AudioDataset(Xval)\n\u001b[0;32m     88\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03mIterates over each row in the DataFrame `df`, extracts the file path and quality label,\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mcomputes the MFCC features for the audio file at the given path, and appends the \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m'file' contains paths to audio files and 'quality' contains corresponding quality labels.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mgetFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mappend(mfcc)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mgetFeatures\u001b[1;34m(path, max_len)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetFeatures\u001b[39m(path, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m259\u001b[39m):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    Extracts MFCC (Mel-frequency cepstral coefficients) features from an audio file.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    numpy.ndarray: A 2D array of MFCC features with shape (n_mfcc, max_len).\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     soundArr, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m lb\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39msoundArr, sr\u001b[38;5;241m=\u001b[39msample_rate)\n\u001b[0;32m     45\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m pad_or_truncate(mfcc, max_len)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/rachelwang/Downloads/bids_with_sensitive_recordings/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab/ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8/audio/sub-2af5afbc-82b1-4656-a203-a8d29b69d3ab_ses-E7F3C75F-1ACC-456F-AABC-8F77F36DBCF8_Diadochokinesis_rec-Diadochokinesis-buttercup.wav'"
     ]
    }
   ],
   "source": [
    "# Load the annotated data\n",
    "annotated_data = pd.read_csv('C:/Users/krish/Desktop/B2AI/b2ai-voice-project1/modified_data_corrected.csv')\n",
    "\n",
    "# Split the data\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(\n",
    "    annotated_data, annotated_data.quality, stratify=annotated_data.quality, random_state=42, test_size=0.25)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "ytrain = le.fit_transform(ytrain)\n",
    "yval = le.transform(yval)\n",
    "\n",
    "# Feature extraction function\n",
    "def pad_or_truncate(feature, max_len):\n",
    "    \"\"\"\n",
    "    Pads or truncates a 2D numpy array to a specified maximum length along its second axis.\n",
    "\n",
    "    Parameters:\n",
    "    feature (numpy.ndarray): The input 2D numpy array to be padded or truncated.\n",
    "    max_len (int): The target length for the second axis of the array.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The modified array, padded with zeros or truncated to the specified length.\n",
    "    \"\"\"\n",
    "    if feature.shape[1] < max_len:\n",
    "        pad_width = max_len - feature.shape[1]\n",
    "        feature = np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        feature = feature[:, :max_len]\n",
    "    return feature\n",
    "\n",
    "def getFeatures(path, max_len=259):\n",
    "    \"\"\"\n",
    "    Extracts MFCC (Mel-frequency cepstral coefficients) features from an audio file.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the audio file.\n",
    "    max_len (int, optional): The maximum length for the MFCC features. Default is 259.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 2D array of MFCC features with shape (n_mfcc, max_len).\n",
    "    \"\"\"\n",
    "    soundArr, sample_rate = lb.load(path)\n",
    "    mfcc = lb.feature.mfcc(y=soundArr, sr=sample_rate)\n",
    "    mfcc = pad_or_truncate(mfcc, max_len)\n",
    "    return mfcc\n",
    "\n",
    "# Custom dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            \"\"\"\n",
    "            Iterates over each row in the DataFrame `df`, extracts the file path and quality label,\n",
    "            computes the MFCC features for the audio file at the given path, and appends the \n",
    "            features and labels to their respective lists in the class instance.\n",
    "\n",
    "            Parameters:\n",
    "            idx (int): The index of the current row.\n",
    "            row (pd.Series): The current row in the DataFrame containing file path and quality label.\n",
    "\n",
    "            Attributes:\n",
    "            self.features (list): A list to store the extracted MFCC features.\n",
    "            self.labels (list): A list to store the quality labels corresponding to each audio file.\n",
    "\n",
    "            The function assumes that the DataFrame `df` has columns 'file' and 'quality', where\n",
    "            'file' contains paths to audio files and 'quality' contains corresponding quality labels.\n",
    "            \"\"\"\n",
    "            path = row['file']\n",
    "            mfcc = getFeatures(path)\n",
    "            self.features.append(mfcc)\n",
    "            self.labels.append(row['quality'])\n",
    "\n",
    "        self.labels = le.transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = AudioDataset(Xtrain)\n",
    "val_dataset = AudioDataset(Xval)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class MFCCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFCCNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 3), padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 2), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 96, kernel_size=(2, 2), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(96)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(96, 128, kernel_size=(2, 2), padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, 8)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): The input tensor with shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after passing through the neural network layers.\n",
    "\n",
    "        The forward pass consists of the following steps:\n",
    "        1. Convolutional layer 1: Applies a convolution, batch normalization, ReLU activation, and pooling.\n",
    "        2. Convolutional layer 2: Applies a convolution, batch normalization, ReLU activation, and pooling.\n",
    "        3. Convolutional layer 3: Applies a convolution, batch normalization, ReLU activation, and pooling.\n",
    "        4. Convolutional layer 4: Applies a convolution, batch normalization, ReLU activation, and global max pooling.\n",
    "        5. Flattening: Reshapes the tensor to (batch_size, -1).\n",
    "        6. Fully connected layer 1: Applies a linear transformation, ReLU activation, and dropout.\n",
    "        7. Fully connected layer 2: Applies a linear transformation, ReLU activation, and dropout.\n",
    "        8. Output layer: Applies a final linear transformation.\n",
    "\n",
    "        Note:\n",
    "        - The `nn.ReLU()` activation function is used after each convolutional and fully connected layer except the last one.\n",
    "        - The `self.pool1`, `self.pool2`, and `self.pool3` are pooling layers.\n",
    "        - The `self.gmp` is a global max pooling layer.\n",
    "        - The `self.bn1`, `self.bn2`, `self.bn3`, and `self.bn4` are batch normalization layers.\n",
    "        - The `self.conv1`, `self.conv2`, `self.conv3`, and `self.conv4` are convolutional layers.\n",
    "        - The `self.fc1`, `self.fc2`, and `self.fc3` are fully connected layers.\n",
    "        - The `self.dropout` applies dropout regularization to prevent overfitting.\n",
    "        \"\"\"\n",
    "        x = self.pool1(nn.ReLU()(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(nn.ReLU()(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(nn.ReLU()(self.bn3(self.conv3(x))))\n",
    "        x = self.gmp(nn.ReLU()(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(nn.ReLU()(self.fc1(x)))\n",
    "        x = self.dropout(nn.ReLU()(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MFCCNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_patience = 50\n",
    "early_stop_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# File path for saving the best model\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\"\n",
    "    Iterate over the training process for a specified number of epochs.\n",
    "\n",
    "    Each epoch involves training the model with the training dataset and evaluating it\n",
    "    with the validation dataset. It also includes early stopping based on the validation loss.\n",
    "\n",
    "    Parameters:\n",
    "    - num_epochs (int): The total number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        \"\"\"\n",
    "        Iterate over the training dataset to perform a forward pass, compute loss,\n",
    "        perform backpropagation, and update the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - train_loader (DataLoader): The DataLoader object for the training dataset.\n",
    "        \"\"\"\n",
    "        mfcc = torch.tensor(features).unsqueeze(1).float().to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mfcc)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \"\"\"\n",
    "        Iterate over the validation dataset to evaluate the model's performance.\n",
    "\n",
    "        Parameters:\n",
    "        - val_loader (DataLoader): The DataLoader object for the validation dataset.\n",
    "        \"\"\"\n",
    "        for features, labels in val_loader:\n",
    "            mfcc = torch.tensor(features).unsqueeze(1).float().to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            outputs = model(mfcc)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        \"\"\"\n",
    "        Check if the current validation loss is the best seen so far. If so, save the model's\n",
    "        state and reset the early stopping counter. Otherwise, increment the counter and check\n",
    "        if early stopping should be triggered.\n",
    "\n",
    "        Parameters:\n",
    "        - best_val_loss (float): The best validation loss observed so far.\n",
    "        - early_stop_counter (int): Counter for early stopping patience.\n",
    "        - early_stop_patience (int): The patience for early stopping.\n",
    "        - best_model_path (str): The file path to save the best model.\n",
    "        \"\"\"\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
